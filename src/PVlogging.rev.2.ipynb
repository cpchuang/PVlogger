{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "e4ed6692-0fb8-4d86-abb6-8bd6a9b43833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main and sub-functions section\n",
    "# TODO: \n",
    "#    1. use SDDI format instead of csv (check this out, https://pylhc.github.io/sdds/index.html)\n",
    "\n",
    "import urllib, datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "\n",
    "def get_group():\n",
    "    \"\"\"\n",
    "    this function get Datagroup list, no input needed.\n",
    "    \n",
    "    Usage: \n",
    "        group_list = get_group()\n",
    "    \"\"\"\n",
    "    \n",
    "    # site and query info\n",
    "    site = 'https://ops.aps.anl.gov/cgi-bin/oagMonitorDataReview.cgi'\n",
    "    query = {'ListDataGroups': '1'}\n",
    "    # constuct url\n",
    "    url = f'{site}?{urllib.parse.urlencode(query)}'\n",
    "    #print(url)\n",
    "    \n",
    "    # get the page\n",
    "    resp = urllib.request.urlopen(url)\n",
    "    the_page = resp.read()\n",
    "    resp.close()\n",
    "    # parse the page\n",
    "    # see this page \"https://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-a-parser\" for parser selection\n",
    "    soup= BeautifulSoup(the_page, 'lxml')\n",
    "    # get Data_Group list\n",
    "    group_list = soup.body.get_text().split()\n",
    "    \n",
    "    return group_list\n",
    "\n",
    "\n",
    "def get_PVlist(datagroup):\n",
    "    \"\"\"\n",
    "    this function get List of PVs under a specific DataGroup\n",
    "    \n",
    "    Usage:\n",
    "        PV_list = get_PVlist(datagroup)\n",
    "        datagroup must be a string\n",
    "    \"\"\"\n",
    "    if not isinstance(datagroup,(list, tuple, str)):\n",
    "        raise TypeError(\"Input must be a tuple or list or string\")\n",
    "    elif isinstance(datagroup,(list, tuple)) and len(datagroup)>1 :\n",
    "        print('Input has more than one element, process only the first one!!')\n",
    "        datagroup=datagroup[0]\n",
    "    \n",
    "    # site and query info\n",
    "    site = 'https://ops.aps.anl.gov/cgi-bin/oagMonitorDataReview.cgi'\n",
    "    query = {'DataGroup': datagroup,\n",
    "             'ListReadbackNames': '1',}\n",
    "    # constuct url\n",
    "    url = f'{site}?{urllib.parse.urlencode(query)}'\n",
    "\n",
    "    # get the page\n",
    "    resp = urllib.request.urlopen(url)\n",
    "    the_page = resp.read()\n",
    "    resp.close()\n",
    "    # parse the page\n",
    "    # see this page \"https://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-a-parser\" for parser selection\n",
    "    soup= BeautifulSoup(the_page, 'lxml')\n",
    "    # get Data_Group list\n",
    "    PV_list = soup.body.get_text().split()\n",
    "\n",
    "    if not PV_list:\n",
    "        print(f'The requested DataGroup \"{datagroup}\" does not exist!!')\n",
    "    else:\n",
    "        return PV_list\n",
    "\n",
    "    \n",
    "def get_PVdata(datagroup, pvlist, start_time='24', end_time='now'):\n",
    "    \"\"\"\n",
    "    This function get PVdata of multiple PVs within the same datagroup\n",
    "    \n",
    "    Usage:\n",
    "        header, data = get_PVdata(datagroup, pvlist, start_time='24', end_time='now')\n",
    "        \n",
    "         datagroup: datagroup name (str)\n",
    "            pvlist: list of PV_name(ReadBackNames on Data Review website, not necessary PV_name)\n",
    "        start_time: follow YYYY/MM/DD format, or 'h' for \"h\" hours before end_time. default: 24 (hours)\n",
    "          end_time: follow YYYY/MM/DD format, or 'h' for \"h\" hours after start_time, or 'now'. default: 'now'\n",
    "    \"\"\"\n",
    "    \n",
    "    if not isinstance(datagroup,(list, tuple, str)):\n",
    "        raise TypeError(\"Input must be a tuple or list or string\")\n",
    "    elif isinstance(datagroup,(list, tuple)) and len(datagroup)>1 :\n",
    "        print('Input more than one DataGroup, query only the first one!!')\n",
    "        datagroup=datagroup[0]        \n",
    "\n",
    "    # get PVlist from Datagroup\n",
    "    PVlist = get_PVlist(datagroup)\n",
    "    #print(PVlist)\n",
    "    \n",
    "    print(pvlist)\n",
    "\n",
    "    index_list = []\n",
    "    if pvlist=='all':\n",
    "        index_list = range(0,len(PVlist))\n",
    "    else:\n",
    "        for pv in pvlist:\n",
    "            index_list.append(PVlist.index(pv))\n",
    "\n",
    "    if end_time=='now':\n",
    "        endtime  = datetime.datetime.now()\n",
    "        endyear  = endtime.year\n",
    "        endmonth = endtime.month\n",
    "        endday   = endtime.day\n",
    "        endhour  = endtime.hour\n",
    "        if start_time.isdigit():\n",
    "            #print(f'start_time is digit and is {start_time}')\n",
    "            starttime=endtime-datetime.timedelta(hours=int(start_time))\n",
    "        else:\n",
    "            if len(start_time.split('/'))==2:\n",
    "                start_time = f'{datetime.date.today().year}/'+start_time\n",
    "            starttime = datetime.datetime.strptime(start_time,'%Y/%m/%d')\n",
    "\n",
    "        startyear  = starttime.year\n",
    "        startmonth = starttime.month\n",
    "        startday   = starttime.day\n",
    "        starthour  = 0\n",
    "    else:\n",
    "        if len(start_time.split('/'))==2:\n",
    "            start_time = f'{datetime.date.today().year}/'+start_time\n",
    "        starttime = datetime.datetime.strptime(start_time,'%Y/%m/%d')\n",
    "\n",
    "        if end_time.isdigit():\n",
    "            endtime=starttime+datetime.timedelta(hours=int(end_time))\n",
    "        else:\n",
    "            if len(end_time.split('/'))==2:\n",
    "                end_time = f'{datetime.date.today().year}/'+end_time\n",
    "            endtime = datetime.datetime.strptime(end_time,'%Y/%m/%d')\n",
    "\n",
    "        startyear  = starttime.year\n",
    "        startmonth = starttime.month\n",
    "        startday   = starttime.day\n",
    "        starthour  = 0\n",
    "        endyear  = endtime.year\n",
    "        endmonth = endtime.month\n",
    "        endday   = endtime.day\n",
    "        endhour  = endtime.hour\n",
    "    #print(f'start date is {starttime}')\n",
    "    #print(f'  end date is {endtime}')\n",
    "\n",
    "    # site and query info\n",
    "    site = 'https://ops.aps.anl.gov/cgi-bin/oagMonitorDataReview.cgi'\n",
    "\n",
    "    query = {'DataGroup': datagroup,\n",
    "             datagroup+'_ControlReadbackName': index_list,\n",
    "            'ExportCSV': 'Export+Data+(CSV)',\n",
    "            'StartYear':  startyear,\n",
    "            'StartMonth': startmonth,\n",
    "            'StartDay':   startday,\n",
    "            'StartHour':  starthour,\n",
    "            'EndYear':    endyear,\n",
    "            'EndMonth':   endmonth,\n",
    "            'EndDay':     endday,\n",
    "            'EndHour':    endhour,}\n",
    "\n",
    "\n",
    "    # constuct url\n",
    "    url = f'{site}?{urllib.parse.urlencode(query,True)}'\n",
    "    # need this freaking line to parse address correctly(\"+\" & \"()\" sign)!! maybe there is a way to do it in erlencode() but I cannot figure it out!!\n",
    "    url = urllib.parse.unquote_plus(url)\n",
    "    print(url)\n",
    "\n",
    "    resp = urllib.request.urlopen(url)\n",
    "    #print(resp.status)\n",
    "\n",
    "    # treat as a html page\n",
    "    the_page = resp.read()\n",
    "    soup= BeautifulSoup(the_page, 'lxml')\n",
    "    resp.close()\n",
    "\n",
    "    try:\n",
    "        header = soup.title.get_text()\n",
    "        if 'Error' in header:\n",
    "            print(f'Respond: {header}')\n",
    "            print(f'{soup.body.get_text()}')\n",
    "            arr=None\n",
    "        else:\n",
    "            print(f'Undefined Respond: {header}')\n",
    "            arr=None\n",
    "    except:\n",
    "        # title is None, so likely a text file\n",
    "        print('Data download successfully...')\n",
    "        body = soup.body.get_text().rstrip().split()\n",
    "        header = body[1].split(',')\n",
    "        arr=np.array([])\n",
    "        for i in range(2,len(body)):\n",
    "            da_=body[i].rstrip().split(',')\n",
    "            if not da_[0]=='':   # skip empty line\n",
    "                arr = np.append(arr, np.array(da_).astype('float'))\n",
    "                # reshape array, now we have the data\n",
    "                arr = np.reshape(arr,(-1,len(header)))   \n",
    "\n",
    "    return header, arr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "6add59fc-c08f-4811-bd7a-29036890cfd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ID:6:tc32:TC32:Ti0', 'ID:6:tc32:TC32:Ti2', 'ID:6:tc32:TC32:Ti8']\n",
      "https://ops.aps.anl.gov/cgi-bin/oagMonitorDataReview.cgi?DataGroup=6ID&6ID_ControlReadbackName=0&6ID_ControlReadbackName=2&6ID_ControlReadbackName=8&ExportCSV=Export+Data+(CSV)&StartYear=2022&StartMonth=10&StartDay=11&StartHour=0&EndYear=2022&EndMonth=10&EndDay=14&EndHour=0\n",
      "Data download successfully...\n",
      "['CAerrors', 'Time', 'ID:6:tc32:TC32:Ti0', 'ID:6:tc32:TC32:Ti2', 'ID:6:tc32:TC32:Ti8']\n"
     ]
    }
   ],
   "source": [
    "#### get the data\n",
    "PVgroup='6ID'\n",
    "#PVlist=['S:SRcurrentAI']\n",
    "PVlist=['ID:6:tc32:TC32:Ti0','ID:6:tc32:TC32:Ti2','ID:6:tc32:TC32:Ti8']\n",
    "#PVlist='all'\n",
    "\n",
    "header, arr = get_PVdata(PVgroup, PVlist, '72')\n",
    "print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "756dbeee-ccc2-44f5-a188-a122c1707a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Plot the result\n",
    "\n",
    "## convert epoch time to human readable time\n",
    "#import datetime\n",
    "\n",
    "eptime = arr[:,1]\n",
    "\n",
    "## plot result\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "%matplotlib qt5\n",
    "#%matplotlib inline\n",
    "\n",
    "#humantime = mdate.date2num(datetime.utcfromtimestamp(eptime)) \n",
    "\n",
    "fig,ax=plt.subplots()\n",
    "for i in range(2, len(header)):\n",
    "    ax.plot(eptime.astype('datetime64[s]'), arr[:,i],'-o', label=header[i])\n",
    "\n",
    "# set proper timezone to display correct time. \n",
    "ax.xaxis_date(tz='US/Central')\n",
    "\n",
    "plt.xlabel('Time (sec)',fontsize=15)\n",
    "plt.ylabel('Temperature(degC)', fontsize=15)\n",
    "ax.legend()\n",
    "fig.set_size_inches(12, 8)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2eef8e-7710-437a-8c92-e02a3f6da3c1",
   "metadata": {},
   "source": [
    "# Development section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "b0c88c09-9f3d-45d4-a1c2-0b911afd19e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start date is 2022-10-05 00:00:00\n",
      "  end date is 2022-10-05 08:00:00\n",
      "https://ops.aps.anl.gov/cgi-bin/oagMonitorDataReview.cgi?DataGroup=6ID&6ID_ControlReadbackName=0&ExportCSV=Export+Data+(CSV)&StartYear=2022&StartMonth=10&StartDay=5&StartHour=0&EndYear=2022&EndMonth=10&EndDay=5&EndHour=8\n",
      "Data download successfully...\n",
      "['CAerrors', 'Time', 'ID:6:tc32:TC32:Ti0']\n",
      "(450, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import datetime\n",
    "\n",
    "datagroup='6ID'\n",
    "#pvlist = ['S:SRcurrentAI','ID01DSEnergy']\n",
    "pvlist = ['ID:6:tc32:TC32:Ti0']\n",
    "PVlist=get_PVlist(datagroup)\n",
    "start_time='10/5'\n",
    "end_time='8'\n",
    "#####\n",
    "\n",
    "index_list = []\n",
    "for pv in pvlist:\n",
    "    index_list.append(PVlist.index(pv))\n",
    "\n",
    "    \n",
    "if end_time=='now':\n",
    "    endtime  = datetime.datetime.now()\n",
    "    endyear  = endtime.year\n",
    "    endmonth = endtime.month\n",
    "    endday   = endtime.day\n",
    "    endhour  = endtime.hour\n",
    "    if start_time.isdigit():\n",
    "        print(f'start_time is digit and is {start_time}')\n",
    "        starttime=endtime-datetime.timedelta(hours=int(start_time))\n",
    "    else:\n",
    "        if len(start_time.split('/'))==2:\n",
    "            start_time = f'{datetime.date.today().year}/'+start_time\n",
    "        starttime = datetime.datetime.strptime(start_time,'%Y/%m/%d')\n",
    "        \n",
    "    startyear  = starttime.year\n",
    "    startmonth = starttime.month\n",
    "    startday   = starttime.day\n",
    "    starthour  = 0\n",
    "else:\n",
    "    if len(start_time.split('/'))==2:\n",
    "        start_time = f'{datetime.date.today().year}/'+start_time\n",
    "    starttime = datetime.datetime.strptime(start_time,'%Y/%m/%d')\n",
    "\n",
    "    if end_time.isdigit():\n",
    "        endtime=starttime+datetime.timedelta(hours=int(end_time))\n",
    "    else:\n",
    "        if len(end_time.split('/'))==2:\n",
    "            end_time = f'{datetime.date.today().year}/'+end_time\n",
    "        endtime = datetime.datetime.strptime(end_time,'%Y/%m/%d')\n",
    "\n",
    "    startyear  = starttime.year\n",
    "    startmonth = starttime.month\n",
    "    startday   = starttime.day\n",
    "    starthour  = 0\n",
    "    endyear  = endtime.year\n",
    "    endmonth = endtime.month\n",
    "    endday   = endtime.day\n",
    "    endhour  = endtime.hour\n",
    "    \n",
    "print(f'start date is {starttime}')\n",
    "print(f'  end date is {endtime}')\n",
    "        \n",
    "            \n",
    "# site and query info\n",
    "site = 'https://ops.aps.anl.gov/cgi-bin/oagMonitorDataReview.cgi'\n",
    "\n",
    "query = {'DataGroup': datagroup,\n",
    "         datagroup+'_ControlReadbackName': index_list,\n",
    "        'ExportCSV': 'Export+Data+(CSV)',\n",
    "        'StartYear':  startyear,\n",
    "        'StartMonth': startmonth,\n",
    "        'StartDay':   startday,\n",
    "        'StartHour':  starthour,\n",
    "        'EndYear':    endyear,\n",
    "        'EndMonth':   endmonth,\n",
    "        'EndDay':     endday,\n",
    "        'EndHour':    endhour,}\n",
    "\n",
    "\n",
    "# constuct url\n",
    "url = f'{site}?{urllib.parse.urlencode(query,True)}'\n",
    "# need this freaking line to parse address correctly(\"+\" & \"()\" sign)!! maybe there is a way to do it in erlencode() but I cannot figure it out!!\n",
    "url = urllib.parse.unquote_plus(url)\n",
    "print(url)\n",
    "\n",
    "resp = urllib.request.urlopen(url)\n",
    "#print(resp.status)\n",
    "\n",
    "# treat as a html page\n",
    "the_page = resp.read()\n",
    "soup= BeautifulSoup(the_page, 'lxml')\n",
    "resp.close()\n",
    "\n",
    "try:\n",
    "    header = soup.title.get_text()\n",
    "    if 'Error' in header:\n",
    "        print(f'Respond: {header}')\n",
    "        print(f'{soup.body.get_text()}')\n",
    "        arr=None\n",
    "    else:\n",
    "        print(f'Undefined Respond: {header}')\n",
    "        arr=None\n",
    "except:\n",
    "    # title is None, so likely a text file\n",
    "    print('Data download successfully...')\n",
    "    body = soup.body.get_text().rstrip().split()\n",
    "    header = body[1].split(',')\n",
    "    arr=np.array([])\n",
    "    for i in range(2,len(body)):\n",
    "        da_=body[i].rstrip().split(',')\n",
    "        if not da_[0]=='':   # skip empty line\n",
    "            arr = np.append(arr, np.array(da_).astype('float'))\n",
    "            # reshape array, now we have the data\n",
    "            arr = np.reshape(arr,(-1,len(header)))   \n",
    "\n",
    "\n",
    "print(header)\n",
    "print(arr.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "80814e66-3285-46a5-80de-26f6dec6eb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.31\n"
     ]
    }
   ],
   "source": [
    "print(fig.get_figwidth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd4b89e-4329-4469-a2cd-0145af6a6a40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
