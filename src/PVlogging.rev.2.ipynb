{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "e4ed6692-0fb8-4d86-abb6-8bd6a9b43833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main and sub-functions section\n",
    "# TODO: \n",
    "#    1. use SDDI format instead of csv (check this out, https://pylhc.github.io/sdds/index.html)\n",
    "\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "\n",
    "def get_group():\n",
    "    # this function get Datagroup list\n",
    "    \n",
    "    # site and query info\n",
    "    site = 'https://ops.aps.anl.gov/cgi-bin/oagMonitorDataReview.cgi'\n",
    "    query = {'ListDataGroups': '1'}\n",
    "    # constuct url\n",
    "    url = f'{site}?{urllib.parse.urlencode(query)}'\n",
    "    \n",
    "    print(url)\n",
    "    \n",
    "    # get the page\n",
    "    resp = urllib.request.urlopen(url)\n",
    "    the_page = resp.read()\n",
    "    resp.close()\n",
    "    # parse the page\n",
    "    # see this page \"https://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-a-parser\" for parser selection\n",
    "    soup= BeautifulSoup(the_page, 'lxml')\n",
    "    # get Data_Group list\n",
    "    group_list = soup.body.get_text().split()\n",
    "    \n",
    "    return group_list\n",
    "\n",
    "\n",
    "def get_PVlist(datagroup):\n",
    "    # this function get List of PVs under a specific DataGroup\n",
    "    # DataGroup must be a list or tuple and the function only process the first element (for now)\n",
    "    \n",
    "    if not isinstance(datagroup,(list, tuple, str)):\n",
    "        raise TypeError(\"Input must be a tuple or list or string\")\n",
    "    elif isinstance(datagroup,(list, tuple)) and len(datagroup)>1 :\n",
    "        print('Input has more than one element, process only the first one!!')\n",
    "        datagroup=datagroup[0]\n",
    "\n",
    "    # site and query info\n",
    "    site = 'https://ops.aps.anl.gov/cgi-bin/oagMonitorDataReview.cgi'\n",
    "    query = {'DataGroup': datagroup,\n",
    "             'ListReadbackNames': '1',}\n",
    "    # constuct url\n",
    "    url = f'{site}?{urllib.parse.urlencode(query)}'\n",
    "\n",
    "    # get the page\n",
    "    resp = urllib.request.urlopen(url)\n",
    "    the_page = resp.read()\n",
    "    resp.close()\n",
    "    # parse the page\n",
    "    # see this page \"https://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-a-parser\" for parser selection\n",
    "    soup= BeautifulSoup(the_page, 'lxml')\n",
    "    # get Data_Group list\n",
    "    PV_list = soup.body.get_text().split()\n",
    "\n",
    "    if not PV_list:\n",
    "        print(f'The requested DataGroup \"{datagroup}\" does not exist!!')\n",
    "    else:\n",
    "        return PV_list\n",
    "\n",
    "def get_PVdata(datagroup, pvlist, date_range=None):\n",
    "\n",
    "    if not isinstance(datagroup,(list, tuple, str)):\n",
    "        raise TypeError(\"Input must be a tuple or list or string\")\n",
    "    elif isinstance(datagroup,(list, tuple)) and len(datagroup)>1 :\n",
    "        print('Input more than one DataGroup, query only the first one!!')\n",
    "        datagroup=datagroup[0]        \n",
    "\n",
    "    PVlist=get_PVlist(datagroup)\n",
    "    #print(PVlist)\n",
    "        \n",
    "    # site and query info\n",
    "    site = 'https://ops.aps.anl.gov/cgi-bin/oagMonitorDataReview.cgi'\n",
    "    query = {'DataGroup': datagroup,\n",
    "             datagroup+'_ControlReadbackName': '18',\n",
    "            'ExportCSV': 'Export+Data+(CSV)',\n",
    "            'StartYear': '2022',\n",
    "            'StartMonth':'10',\n",
    "            'StartDay':  '11',\n",
    "            'StartHour': '0',\n",
    "            'EndYear':   '2022',\n",
    "            'EndMonth':  '10',\n",
    "            'EndDay':    '11',\n",
    "            'EndHour':   '24',}\n",
    "    # constuct url\n",
    "    url = f'{site}?{urllib.parse.urlencode(query)}'\n",
    "    # need this freaking line to parse address correctly!! maybe there is a way to do it in erlencode() but I cannot figure it out!!\n",
    "    url = urllib.parse.unquote_plus(url)\n",
    "\n",
    "    #print(url)\n",
    "    # get the data (csv format)\n",
    "    resp = urllib.request.urlopen(url)\n",
    "    #print(resp.status)\n",
    "    lines = [l.decode('utf-8') for l in resp.readlines()]\n",
    "    resp.close()\n",
    "\n",
    "    # header\n",
    "    header = lines[2].rstrip().split(',')\n",
    "\n",
    "    # read data\n",
    "    arr=np.array([])\n",
    "    for i in range(3,len(lines)):\n",
    "        da_=lines[i].rstrip().split(',')\n",
    "    if not da_[0]=='':   # skip empty line\n",
    "        arr = np.append(arr, np.array(da_).astype('float'))\n",
    "\n",
    "    # reshape array, now we have the data\n",
    "    arr = np.reshape(arr,(-1,len(header)))    \n",
    "\n",
    "    return header, arr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "6add59fc-c08f-4811-bd7a-29036890cfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### get the data\n",
    "PVgroup=('1ID')\n",
    "PVlist=('S:SRcurrentAI')\n",
    "\n",
    "header, da = get_PVdata(PVgroup, PVlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "756dbeee-ccc2-44f5-a188-a122c1707a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Plot the result\n",
    "\n",
    "## convert epoch time to human readable time\n",
    "#import datetime\n",
    "\n",
    "eptime = arr[:,1]\n",
    "\n",
    "## plot result\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "%matplotlib qt5\n",
    "#%matplotlib inline\n",
    "\n",
    "#humantime = mdate.date2num(datetime.utcfromtimestamp(eptime)) \n",
    "\n",
    "fig,ax=plt.subplots()\n",
    "for i in range(2, len(header)):\n",
    "    ax.plot(eptime.astype('datetime64[s]'), arr[:,i],'o', label=header[i])\n",
    "\n",
    "# set proper timezone to display correct time. \n",
    "ax.xaxis_date(tz='US/Central')\n",
    "\n",
    "plt.xlabel('Time (sec)')\n",
    "plt.ylabel('Temperature(degC)')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "b0c88c09-9f3d-45d4-a1c2-0b911afd19e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'isdigit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [391], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m     endday   \u001b[38;5;241m=\u001b[39m endtime\u001b[38;5;241m.\u001b[39mday\n\u001b[1;32m     18\u001b[0m     endhour  \u001b[38;5;241m=\u001b[39m endtime\u001b[38;5;241m.\u001b[39mhour\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m start_time\u001b[38;5;241m.\u001b[39misdigit():\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_time is digit and is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# site and query info\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'isdigit'"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "datagroup='1ID'\n",
    "pvlist = ['S:SRcurrentAI','ID01DSEnergy']\n",
    "PVlist=get_PVlist(datagroup)\n",
    "start_time=5\n",
    "end_time='now'\n",
    "#####\n",
    "\n",
    "index_list = []\n",
    "for pv in pvlist:\n",
    "    index_list.append(PVlist.index(pv))\n",
    "\n",
    "if end_time=='now':\n",
    "    endtime  = datetime.now()\n",
    "    endyear  = endtime.year\n",
    "    endmonth = endtime.month\n",
    "    endday   = endtime.day\n",
    "    endhour  = endtime.hour\n",
    "    if start_time.isdigit():\n",
    "        print(f'start_time is digit and is {start_time}')\n",
    "    \n",
    "\n",
    "# site and query info\n",
    "site = 'https://ops.aps.anl.gov/cgi-bin/oagMonitorDataReview.cgi'\n",
    "\n",
    "query = {'DataGroup': datagroup,\n",
    "         datagroup+'_ControlReadbackName': index_list,\n",
    "        'ExportCSV': 'Export+Data+(CSV)',\n",
    "        'StartYear': '2022',\n",
    "        'StartMonth':'10',\n",
    "        'StartDay':  '11',\n",
    "        'StartHour': '0',\n",
    "        'EndYear':   endyear,\n",
    "        'EndMonth':  endmonth,\n",
    "        'EndDay':    endday,\n",
    "        'EndHour':   endhour,}\n",
    "\n",
    "\n",
    "# constuct url\n",
    "url = f'{site}?{urllib.parse.urlencode(query,True)}'\n",
    "# need this freaking line to parse address correctly(\"+\" & \"()\" sign)!! maybe there is a way to do it in erlencode() but I cannot figure it out!!\n",
    "url = urllib.parse.unquote_plus(url)\n",
    "print(url)\n",
    "\n",
    "resp = urllib.request.urlopen(url)\n",
    "#print(resp.status)\n",
    "\n",
    "# treat as a html page\n",
    "the_page = resp.read()\n",
    "soup= BeautifulSoup(the_page, 'lxml')\n",
    "resp.close()\n",
    "\n",
    "try:\n",
    "    header = soup.title.get_text()\n",
    "    if 'Error' in header:\n",
    "        print(f'Respond: {header}')\n",
    "        arr=None\n",
    "    else:\n",
    "        print(f'Undefined Respond: {header}')\n",
    "        arr=None\n",
    "except:\n",
    "    # title is None, so likely a text file\n",
    "    print('Data download successfully...')\n",
    "    body = soup.body.get_text().rstrip().split()\n",
    "    header = body[1].split(',')\n",
    "    arr=np.array([])\n",
    "    for i in range(2,len(body)):\n",
    "        da_=body[i].rstrip().split(',')\n",
    "        if not da_[0]=='':   # skip empty line\n",
    "            arr = np.append(arr, np.array(da_).astype('float'))\n",
    "            # reshape array, now we have the data\n",
    "            arr = np.reshape(arr,(-1,len(header)))   \n",
    "\n",
    "\n",
    "print(header)\n",
    "#print(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "80814e66-3285-46a5-80de-26f6dec6eb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(int(2.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb12eae-b554-4c3b-a59d-55c0ced8dde7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
